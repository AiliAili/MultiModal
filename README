As the dataset is too large to upload into github.
You can get access to the dataset used in our paper via the following link.

https://drive.google.com/open?id=19L8l1J1EiW8aWaB4IXcZXpUhll9zAhsA
or
https://drive.google.com/drive/folders/1v_i4yxeK6BLQe5mcQfxWr23WC2EBVWXq

Citation:

@inproceedings{Shen:19,
  author    = {Aili Shen and
               Bahar Salehi and
               Timothy Baldwin and
               Jianzhong Qi},
  title     = {A Joint Model for Multimodal Document Quality Assessment},
  booktitle = {19th {ACM/IEEE} Joint Conference on Digital Libraries, {JCDL} 2019,
               Champaign, IL, USA, June 2-6, 2019},
  pages     = {107--110},
  year      = {2019}
}

and 

@article{Shen:20,
  title={A General Approach to Multimodal Document Quality Assessment},
  author={Shen, Aili and Salehi, Bahar and Qi, Jianzhong and Baldwin, Timothy},
  journal={Journal of Artificial Intelligence Research},
  volume={68},
  pages={607--632},
  year={2020}
}


Software version:
keras=2.0.8
tensorflow=1.4.0
python=2.7.12


The code named keras_inception_v3_plus_test.py is to test whether a model works in extracting visual features from screenshots
to run this code, I use the following script: 
"CUDA_VISIBLE_DEVICES=0 python keras_inception_v3_plus_test.py  --img_width 500  --img_height  500  --batch_size 16"

To run bi_lstm_inception.py, a joint model extracting textual and visual features from both sources. We need to make modifications of keras source code:
I modify the source code of keras about the function of "keras.preprocessing.image.ImageDataGenerator.flow_from_directory" to make it working for our joint model. And I also upload my edited version on Github named image.py. You can also use this file to replace yours at the directory of /install_path/lib/python2.7/site-packages/keras/preprocessing/image.py. Basically, I modified the code line of 1016-1067.
To run the joint model, I use the following script:
"CUDA_VISIBLE_DEVICES=1  python  bi_lstm_inception.py   --data_flag 5 --recompute_flag False --train_data_dir  ./training_1000_2000  --validation_data_dir  ./validation_1000_2000  --test_data_dir  ./test_1000_2000  --img_width 500  --img_height  500   --nb_train_samples 24000   --nb_validation_samples 3000   --nb_test_samples 2794  --batch_size  16"

So I recommend you to build two virtual environments, one unmodified version for an inception model to extract visual features. And another one modified as described above to run the joint model.

Stay tuned.
